ID,Name,Header_Affiliation,Author_Type,Credibility_Tier,Primary_Affiliation,Recent_Sources_Raw,Position_Summary,Representative_Quotes_Raw,Key_Arguments,Camp_Categorizations_Raw,Camp_Evolution,AI_Progress_Camp_Label,AI_Progress_Camp_PrimarySecondary,Society_Ethics_Camp_Label,Society_Ethics_Camp_PrimarySecondary,Enterprise_Camp_Label,Enterprise_Camp_PrimarySecondary,Governance_Camp_Label,Governance_Camp_PrimarySecondary,Future_of_Work_Camp_Label,Future_of_Work_Camp_PrimarySecondary,All_URLs,Primary_Work_Title,Primary_Work_Outlet,Primary_Work_Date,Primary_Work_URL
ian_hogarth,Ian Hogarth,"Chair, UK AI Safety Institute; investor",Industry Leader,Thought Leader,UK AI Safety Institute,"Essay 'AI Nationalism' (2018, personal blog); Financial Times columns on AI and geopolitics (2023–2025); Public interviews as UK AI Safety Institute chair (2024–2025)",Frames frontier AI as a 'Godzilla'-scale technology that is both economically transformative and geopolitically destabilising; argues for strong public institutions and international coordination to manage national AI races.,'We are building something closer to a Godzilla than a helpful assistant.'; 'The big question is whether states can build the institutional capacity to govern frontier models.',Frontier AI will concentrate power in a few labs and states; compute and chip supply chains are now strategic assets; governments need technical capacity and evaluation infrastructure; sloppy acceleration creates tail risks that markets under-price.,AI progress: fast but fragile; needs guardrails; state capacity and evals are central; open science should be balanced with security.,"Moved from early-stage AI investor optimism to more governance-centric, safety-aware stance as models scaled and geopolitics heated up.",Cautious Accelerationist,Primary,Democratic Risk Steward,Primary,Strategic Compute Capitalist,Secondary,State Capacity Builder,Primary,Transition Planner,Secondary,https://www.ianhogarth.com/blog/ai-nationalism;https://www.ft.com;https://www.aisafetyinstitute.gov.uk,AI Nationalism,Personal blog,2018-06-01,https://www.ianhogarth.com/blog/ai-nationalism
ajeya_cotra,Ajeya Cotra,"Senior Research Analyst, Open Philanthropy (AI timelines)",Academic/Practitioner,Seminal Thinker,Open Philanthropy,"Report 'Forecasting TAI with Biological Anchors' (2020, updated 2022); biological anchors blog discussions; interviews on AI timelines (Future of Life, AXRP)",Builds a quantitative forecast for transformative AI using 'biological anchors' tied to brain and evolution; argues that transformative AI this century is more likely than not but deeply uncertain.,"'There is a real chance transformative AI is developed in the next few decades, but our uncertainty should be front and center.'; 'Biological anchors turn our ignorance into explicit distributions instead of vibes.'",Use brain and evolution as reference classes for training compute; treat timelines as distributions not point estimates; policy should plan for non-trivial probability of transformative AI by 2040; epistemic humility beats both doom and denial.,AI progress modeled via compute and scaling; longtermist but numerate; emphasises uncertainty and explicit probabilities for timelines.,"Shifted median timeline earlier as frontier labs demonstrated scaling and investment exploded, while still stressing wide uncertainty bands.",Quantitative Timelines Modeler,Primary,Longtermist Risk Planner,Primary,AI Strategy Evidence-Seeker,Secondary,Scenario Planning Advocate,Primary,Automation Probability Forecaster,Secondary,https://forecasting.tai.cotra.com;https://www.openphilanthropy.org;https://futureoflife.org,Forecasting TAI with Biological Anchors,Open Philanthropy (working report),2020-07-01,https://forecasting.tai.cotra.com
stuart_russell,Stuart Russell,"Professor of Computer Science, UC Berkeley; AI safety pioneer",Academic,Seminal Thinker,"University of California, Berkeley",Book 'Human Compatible' (2019); co-author of AI textbook 'Artificial Intelligence: A Modern Approach'; talks at UN and policy forums on beneficial AI,Warns that mis-specified objectives in powerful AI systems create catastrophic failures; argues for 'provably beneficial AI' that treats human preferences as uncertain and corrigible.,"'The problem is not malevolence, it's competence in pursuing the wrong objective.'; 'We need to design machines that are provably beneficial to humans.'","Orthodox reward-maximisation is unsafe at scale; AI systems must be uncertain about human preferences; alignment requires corrigibility, verification, and governance; near-term deployment in critical infrastructure should be cautious.","Alignment-first, specification-focused; safety as a formal and engineering problem tightly linked to policy.","Moved from general AI and textbook work to a leading critic of misaligned objectives as deep learning scaled, leaning into safety and governance after 2015.",Alignment-First Formalist,Primary,Human-Centric Ethicist,Primary,Responsible Automation Advocate,Secondary,Global Safety Governance Voice,Primary,Augmentation-Not-Replacement,Secondary,https://www.humancompatible.ai;https://www.penguinrandomhouse.com/books/566677/human-compatible;https://people.eecs.berkeley.edu/~russell,Human Compatible: Artificial Intelligence and the Problem of Control,Viking / Penguin Random House,2019-10-08,https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell
nick_bostrom,Nick Bostrom,"Professor of Philosophy, Oxford; Director, Future of Humanity Institute",Academic,Seminal Thinker,University of Oxford (Future of Humanity Institute),Book 'Superintelligence' (2014); essays on existential risk and AI control; later writings on AI governance and the 'vulnerable world' hypothesis,"Argues that superintelligent AI could permanently tilt the future, including existential catastrophe; stresses the 'control problem' and one-shot nature of superintelligence deployment.",'The first superintelligence may also be the last invention that humanity ever needs to make.'; 'We are like small children playing with a bomb.',"Superintelligence may be discontinuous and strategic; misaligned systems could end human agency; research and governance must reduce existential risk; society underinvests in low-probability, high-impact tail risks.",Canonical longtermist x-risk framing; AI as a civilisational bet with asymmetric downside.,"Recently softened some rhetoric and engaged more with practical governance, but core concern about x-risk from unaligned superintelligence remains.",Superintelligence Longtermist,Primary,Existential-Risk Ethicist,Primary,Macro-Strategy Architect,Secondary,Global X-Risk Governance,Primary,Job Discontinuity Alarmist,Secondary,https://nickbostrom.com;https://nickbostrom.com/books/superintelligence,"Superintelligence: Paths, Dangers, Strategies",Oxford University Press,2014-07-01,https://nickbostrom.com/books/superintelligence
eliezer_yudkowsky,Eliezer Yudkowsky,"Co-founder, MIRI; AI alignment commentator",Academic/Practitioner,Thought Leader,Machine Intelligence Research Institute,Essay series 'AGI Ruin: A List of Lethalities' (2022); posts and interviews on AI doom timelines; earlier technical alignment work on decision theory and corrigibility,Holds that unaligned AGI built under current incentives will almost certainly kill everyone; believes alignment is far behind capabilities and mainstream actors are unserious about the threat.,"'If anyone builds a sufficiently powerful AI under current conditions, I expect that literally everyone on Earth will die.'; 'You want scientific-revolution levels of insight in alignment before you push the big red button.'","Inner optimisers, deceptive alignment, and fast takeoff are likely; labs will not voluntarily slow; existing governance is too weak; only extreme slowdown, global coordination, or radically new alignment breakthroughs suffice.",Extremely pessimistic x-risk stance; AI race framed as near-certain doom under status quo.,Moved from speculative rationalist writings to more direct activism as scaling accelerated; tone has become increasingly urgent and pessimistic post-GPT-3/4.,Doom-Weighted Alarmist,Primary,Catastrophic-Risk Moralist,Primary,Lab Incentive Critic,Secondary,Hard Nonproliferation Hawk,Primary,Collapse-Before-Upgrade,Secondary,https://www.lesswrong.com;https://intelligence.org,AGI Ruin: A List of Lethalities,LessWrong / MIRI,2022-03-15,https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/agi-ruin-a-list-of-lethalities
suresh_venkatasubramanian,Suresh Venkatasubramanian,"Professor, Brown; former OSTP AI Bill of Rights lead",Academic/Policy Maker,Thought Leader,Brown University; former White House OSTP,"'Blueprint for an AI Bill of Rights' (2022, OSTP); public talks unpacking the framework; academic work on algorithmic fairness","Co-led the US effort to articulate an 'AI Bill of Rights' focused on protecting civil rights, privacy, and human agency; emphasises practical levers and existing law over sci-fi narratives.","'This was always meant to be a blueprint, a vision, a set of values laid out, not a set of regulations.'; 'Systems should work, they shouldn’t discriminate, and they shouldn’t eliminate human interlocutors.'","AI should be governed as part of broader automated systems; fairness, non-discrimination, and notice are baseline; existing regulatory levers can be used; human fallback and contestability are crucial.",Sociotechnical realist framing of AI; civil-rights and administrative law as key battlegrounds.,"Shifted from mostly academic fairness work to high-impact policy work, then back to academia while shaping public discourse.",Sociotechnical Realist,Primary,Civil-Rights Governance,Primary,Responsible Automation in Practice,Secondary,Principles-First Regulator,Primary,Worker and Citizen Protections,Secondary,https://www.whitehouse.gov/ostp/ai-bill-of-rights;https://techpolicy.press;https://cs.brown.edu/people/suresh,Blueprint for an AI Bill of Rights,White House OSTP,2022-10-04,https://www.whitehouse.gov/ostp/ai-bill-of-rights
margaret_mitchell,Margaret Mitchell,"Chief Ethics Scientist, Hugging Face; co-author 'Stochastic Parrots'",Academic/Practitioner,Thought Leader,Hugging Face,"Paper 'On the Dangers of Stochastic Parrots' (FAccT 2021); work on Model Cards; public commentary on ethical AI and labor, data, and power","Critiques the 'scale is all you need' paradigm; argues large LMs embed and amplify harms; emphasises documentation, dataset curation, and worker rights as core to responsible AI.",'AI is neither artificial nor intelligent.'; 'Bigger models trained on everything are not automatically better for people.',"Transparency and documentation (model cards, datasheets) are prerequisites; focus on who benefits, who is harmed, and whose labor/data is exploited; decentralised, community-centric AI beats Big Tech monopoly.",Critical sociotechnical stance; justice and labor oriented; open-source but not naive about power.,"Became more openly critical of Big Tech after exit from Google, channeling that into open-source ethics work at Hugging Face.",Critical Sociotechnical Skeptic,Primary,Justice-Oriented Ethicist,Primary,Open-Source Responsibility,Secondary,Platform Accountability Advocate,Primary,Labor and Data Rights Defender,Secondary,https://dl.acm.org/doi/10.1145/3442188.3445922;https://huggingface.co/blog/model-cards,On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?,FAccT Conference Proceedings,2021-03-01,https://dl.acm.org/doi/10.1145/3442188.3445922
rumman_chowdhury,Rumman Chowdhury,"CEO, Humane Intelligence; former Twitter META lead",Practitioner,Thought Leader,Humane Intelligence,Press and interviews on participatory AI evaluation (2023–2025); US Science Envoy for AI role; coverage of Humane Intelligence red-teaming work,"Advocates for sociotechnical audits and community red-teaming of AI systems; warns that outsourcing human judgment to AI is a 'failure state'; stresses pluralistic, participatory governance.","'AI should be treated as a tool, not as a replacement for human thinking.'; 'Delegating our critical thinking to AI is a failure state.'","Third-party audits, bug bounties, and structured red-teaming are central; affected communities must be in the loop; AGI pursuit without accountability deepens power imbalances; AI literacy should be widespread, not elite.","Participatory, audit-first approach to responsible AI; structural harms over speculative doom.",Moved from in-house responsible AI roles at large platforms to independent auditing and public advocacy after seeing internal governance limits.,Participatory Evaluator,Primary,Human Agency Defender,Primary,Risk-Aware Deployment,Secondary,Audit-First Governance,Primary,Worker and Citizen Voice,Secondary,https://humane-ai.org;https://www.businessinsider.com;https://timesofindia.indiatimes.com,Humane Intelligence: Community-Driven AI Evaluation,Humane Intelligence / public talks,2024-01-01,https://humane-ai.org
judea_pearl,Judea Pearl,"Professor, UCLA; pioneer of causal inference",Academic,Seminal Thinker,"University of California, Los Angeles",Book 'The Book of Why' (2018); technical work on causal graphs and do-calculus; essays on AI and causality,"Argues that true intelligence requires causal models, not just pattern matching; current deep learning systems are 'curve-fitters' lacking understanding and counterfactual reasoning.","'You cannot claim credit for an answer if you do not know what question you answered.'; 'To be intelligent, machines must ask counterfactual questions.'","Causal graphs provide a calculus for explanation, intervention, and counterfactuals; AI systems without explicit causal models will hit ceilings in robustness, explainability, and generalisation; policy and science should adopt causal thinking.",Causal-first methodology; skeptical of scale-only narratives; emphasises explainability and decision quality.,"Has become more critical of 'causal-free' deep learning as hype around LLMs has grown, doubling down on the causal revolution framing.",Causal-First Methodologist,Primary,Explainability and Responsibility,Secondary,Decision-Grade Analytics,Primary,Evidence-Based Policy Analytics,Secondary,"Skill-Shift, not Just Task Automation",Secondary,https://bayes.cs.ucla.edu/WHY;https://ucla.in/2G2cVSk,The Book of Why: The New Science of Cause and Effect,Basic Books,2018-05-15,https://bayes.cs.ucla.edu/WHY
yejin_choi,Yejin Choi,"Professor, University of Washington; Senior Research Director, AI2",Academic,Thought Leader,University of Washington; Allen Institute for AI,TED talk 'Why AI is incredibly smart and shockingly stupid' (2023); research on commonsense reasoning and moral narratives; papers on value-aligned language models,"Highlights that current LLMs are 'incredibly smart and shockingly stupid' because they lack robust commonsense and moral reasoning; pushes for smaller, value-aligned models grounded in human norms.","'Common sense is not so common, especially for AI.'; 'We need AI systems that are not just smart, but also wise.'","Commonsense reasoning and norms are hard, under-valued problems; scaling alone will not fix brittleness; models should be trained to respect social norms; concentration of compute raises power and environmental concerns.","Grounding realist; focuses on commonsense, ethics, and sustainability as correctives to scale.","Evolved from core NLP research to explicit critiques of scale-only progress, linking technical work to societal implications.",Commonsense Builder,Primary,Norms-Aware Ethicist,Primary,Human-Centric NLP,Secondary,Democratic AI Research Ecosystem,Secondary,Reskilling with AI Assistants,Secondary,https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid;https://ai2-website.s3.amazonaws.com/team/yejin-choi.html,Why AI Is Incredibly Smart and Shockingly Stupid,TED Talk,2023-04-28,https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid
lilian_weng,Lilian Weng,"Head of Safety Systems, OpenAI; alignment researcher",Practitioner,Thought Leader,OpenAI,"OpenAI blog posts on alignment, interpretability, and adversarial robustness; safety systems work for frontier models","Designs safety systems, evaluations, and mitigations for frontier models; popularises technical alignment topics through accessible long-form posts.","'Alignment is not a one-shot problem, it's an ongoing process as capabilities evolve.'; 'We need scalable oversight methods that grow with model power.'","Adversarial testing, red-teaming, and scalable oversight are key; alignment is deeply technical but tractable; favours iterative deployment with strong safeguards over moratoria.",Lab-embedded safety perspective; pragmatic and empirical about alignment.,Shifted from core deep learning research to almost exclusively safety-focused work as model capabilities increased.,Lab-Embedded Aligner,Primary,Risk-Aware Technologist,Secondary,Enterprise Safety Partner,Primary,Evaluation-Centric Governance,Secondary,Active Monitoring of Task Impacts,Secondary,https://openai.com/blog;https://lilianweng.github.io,Thoughts on AI Alignment and Safety,OpenAI / personal blog,2023-01-01,https://lilianweng.github.io
abeba_birhane,Abeba Birhane,"Director, AI Accountability Lab, Trinity College Dublin; cognitive scientist",Academic,Thought Leader,Trinity College Dublin,"Paper 'Algorithmic Colonization of Africa' (2020, SCRIPTed); blog essays; work auditing large vision datasets and model harms","Argues that AI development often replicates colonial power dynamics, especially in the Global South; critiques large datasets and surveillance infrastructures that treat people as raw material.",'Western tech monopolies export their values under the guise of neutral AI.'; 'Those most affected by AI systems must be at the center of their design.',"Algorithmic colonialism describes how data extraction and deployment marginalise local communities; dataset audits reveal racism, misogyny, and violence baked into 'foundation' corpora; calls for decolonial, community-rooted AI.",Decolonial critic of AI power structures; Global South and justice-centric lens.,"Evolved from broader cognitive science to a sharp, globally recognised critique of power, data, and colonial logics in AI.",Decolonial Critic,Primary,Justice and Power Analyst,Primary,Context-First AI Design,Secondary,Global South Governance Advocate,Primary,Worker and Community Protections,Secondary,https://script-ed.org/article/algorithmic-colonization-of-africa;https://abebabirhane.wordpress.com,Algorithmic Colonization of Africa,SCRIPTed,2020-08-06,https://script-ed.org/article/algorithmic-colonization-of-africa
deborah_raji,Deborah Raji,"Researcher, Mozilla / University of California; algorithmic justice advocate",Academic/Practitioner,Thought Leader,Mozilla Foundation; University of California,'Gender Shades' facial recognition audit; testimonies on facial recognition bias; articles on algorithmic accountability,Shows that commercial facial recognition misclassifies darker-skinned and female faces at much higher rates; pushes for bans or strict limits on high-risk uses like policing.,"'We cannot treat the harms of AI as unfortunate bugs, they are features of the system we built.'; 'Communities should have a say in whether and how AI is deployed on them.'","Auditing and measurement are prerequisites for justice; some applications may be fundamentally incompatible with civil rights; accountability includes legal remedies, corporate responsibility, and community oversight.",Algorithmic justice framing; concrete harms and civil-rights violations as central.,"Expanded from technical audits to broader governance interventions, lawsuits, and public advocacy as evidence accumulated.",Algorithmic Justice Auditor,Primary,Civil-Rights Advocate,Primary,High-Risk Use Skeptic,Secondary,Ban-or-Regulate Governance,Primary,Worker and Citizen Safeguards,Secondary,https://www.gendershades.org;https://www.debora-raji.com,Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,"Conference on Fairness, Accountability, and Transparency",2018-02-01,https://www.gendershades.org
daron_acemoglu,Daron Acemoglu,"Professor of Economics, MIT; technology and inequality researcher",Academic,Thought Leader,Massachusetts Institute of Technology,"Book 'Power and Progress' (2023, with Simon Johnson); papers on automation and inequality; op-eds on AI and the future of work","Argues that the direction of technological change is not neutral; warns that excessive automation, including AI, can enrich capital while undermining labor unless institutions push toward 'shared prosperity' applications.","'We should be asking not what AI can do, but what it should do for us.'; 'Unrestrained automation risks creating a feudal economy of a few tech lords and many precarious workers.'","Policy and norms can redirect AI from labor-replacing toward task-complementing uses; taxation, bargaining power, and antitrust matter; warns against 'productivity theater' that hides distributive harms.",Shared-prosperity framing of tech; inequality and institutional design as key levers.,Extended long-standing work on institutions and growth into a focused critique of current AI deployment narratives.,Shared-Prosperity Technologist,Secondary,Inequality-Focused Ethicist,Primary,Human-Complementary Enterprise,Primary,Institution-Centric Governance,Primary,Worker Power Advocate,Primary,https://press.princeton.edu/books/hardcover/9780300256684/power-and-progress;https://economics.mit.edu/faculty/acemoglu,Power and Progress: Our Thousand-Year Struggle Over Technology and Prosperity,Princeton University Press,2023-05-16,https://press.princeton.edu/books/hardcover/9780300256684/power-and-progress
cassie_kozyrkov,Cassie Kozyrkov,"CEO, Kozyr; former Chief Decision Scientist, Google",Practitioner,Thought Leader,Kozyr (formerly Data Scientific); ex-Google,"Decision intelligence articles (HBR, Medium); Decoder and DataCamp interviews on decision-making with AI; talks to executives on AI strategy","Founded 'decision intelligence' and argues the bottleneck is not AI answers but human decision-making: framing questions, clarifying values, and managing risk.","'Friends don’t let friends do data science alone.'; 'The hard part isn’t getting answers from AI, it’s knowing which questions matter.'",Distinguishes data-inspired from data-driven decisions; emphasises decision pipelines; AI should augment human judgment; organisations need decision literacy more than more dashboards.,"Enterprise-focused, pragmatic humanist; co-evolution of humans and AI in decision infrastructure.","Moved from internal evangelism at Google to independent advisory, with more emphasis on ethical debt and long-term AI strategy for leaders.",Decision Intelligence Evangelist,Primary,Pragmatic Humanist,Primary,Enterprise AI Strategist,Primary,Executive Governance Coach,Secondary,Skill-Up Workforce Advocate,Secondary,https://www.kozyr.com;https://hbr.org/2019/07/the-first-thing-great-decision-makers-do;https://www.theverge.com/decoder-podcast-with-nilay-patel/703269/cassie-kozyrkov-interview-ai-google-decision-scientist,The First Thing Great Decision Makers Do,Harvard Business Review,2019-07-01,https://hbr.org/2019/07/the-first-thing-great-decision-makers-do
daphne_koller,Daphne Koller,"CEO, insitro; co-founder, Coursera",Academic/Practitioner,Seminal Thinker,insitro; formerly Stanford University,Coursera founding talks; research on probabilistic graphical models; insitro work applying ML to drug discovery,Pioneered probabilistic modeling and translated it into education and biotech; treats AI as a rigorous tool for solving hard real-world problems.,"'AI is not magic, it’s statistics done right at scale.'; 'The real promise of machine learning is in domains where we can combine data with scientific understanding.'","AI’s biggest value is in structured, high-stakes domains like biology and education; emphasises rigorous data, domain expertise, and partnerships; relatively quiet on AGI hype.",Scientific applicationist; impact-focused more than speculative.,Shifted from academic theory to applied entrepreneurship while keeping an evidence-based stance on AI capabilities.,Scientific Applicationist,Primary,Responsible Innovator,Secondary,Vertical AI Builder,Primary,Domain-Specific Governance,Secondary,Reskilling via EdTech,Secondary,https://www.insitro.com;https://cs.stanford.edu/people/koller;https://www.coursera.org,Probabilistic Graphical Models,Stanford / Coursera,2012-01-01,https://www.coursera.org/specializations/probabilistic-graphical-models
nat_friedman,Nat Friedman,"Former CEO, GitHub; AI investor",Industry Leader,Thought Leader,NFDG (investment fund); ex-GitHub,"GitHub Copilot launch communications; investments in AI startups (e.g., Midjourney, Scale); reports of joining Meta AI efforts",Champion of developer-centric AI tooling and open-source ecosystems; sees AI as a new general-purpose platform similar to the internet.,"'Software is eating the world, and AI is eating software.'; 'The most interesting AI work happens where open-source and great product taste meet.'","Rapid deployment of AI tools to developers and founders is net positive; emphasises open models, great UX, and founder-friendly capital; optimistic on disruption and value creation.",Techno-optimist builder; OSS plus product taste as core levers.,"Transitioned from running a large dev platform to backing and advising frontier AI companies, nudging them toward open ecosystems.",Techno-Optimist Builder,Primary,Progress-First Ethicist,Secondary,Founder-Centric Enterprise,Primary,Light-Touch Governance,Secondary,Upskilling Developers,Secondary,https://nat.org;https://github.blog/2021-06-29-introducing-github-copilot-ai-powered-pair-programmer,Introducing GitHub Copilot: Your AI Pair Programmer,GitHub Blog,2021-06-29,https://github.blog/2021-06-29-introducing-github-copilot-ai-powered-pair-programmer
patrick_collison,Patrick Collison,"Co-founder and CEO, Stripe; science and progress essayist",Industry Leader,Thought Leader,Stripe,Essays and interviews on progress studies; Fast Grants initiative; commentary on AI and economic growth,"Argues society should be more ambitious about speeding up scientific and technological progress, including AI, while improving state capacity.","'The world could be much richer and more capable than it is, if we took progress seriously.'; 'We should worry at least as much about under-shooting on innovation as we do about over-shooting.'",Supports high-velocity experimentation; sees AI as one tool in a broader progress agenda; favours pro-innovation regulation that still addresses concrete harms; optimistic about human adaptability.,Progress-studies lens on AI; innovation and state capacity over fear.,"Moved from purely business focus to public intellectual role on progress, including funding AI and science efforts.",Progress Studies Optimist,Primary,Prosperity-Oriented Ethicist,Secondary,Productivity-First Enterprise,Primary,Pro-Innovation Regulation,Secondary,Transition and Reskilling Pragmatist,Secondary,https://patrickcollison.com;https://patrickcollison.com/progress,We Need a New Science of Progress,Personal blog / essays,2019-01-01,https://patrickcollison.com/progress
emad_mostaque,Emad Mostaque,"Founder, Stability AI (former CEO)",Industry Leader,Thought Leader,Stability AI (founder),Launch of Stable Diffusion (2022); interviews on open-source generative models; commentary on decentralising AI,"Promoted the idea that powerful generative models should be widely accessible via open weights, arguing this democratises creativity and reduces Big Tech dominance.","'We want to put this technology into the hands of a billion people.'; 'Open-source AI is the best way to ensure it serves everyone, not just a few big companies.'","Believes diffusion of capabilities is a safeguard against centralised control; emphasises open ecosystems, community governance, and rapid iteration; critics say this underestimates misuse and safety issues.",Open-source maximalist; libertarian-leaning tech ethic.,Stepped down as CEO amid governance challenges but remains symbolic of the open-source generative AI push.,Open-Source Maximalist,Primary,Libertarian Tech Ethicist,Secondary,Creative Economy Booster,Primary,Minimal-Barrier Governance,Secondary,Freelancer and Creator Empowerment,Secondary,https://stability.ai;https://stability.ai/blog/stable-diffusion-public-release,Stable Diffusion Public Release,Stability AI blog,2022-08-22,https://stability.ai/blog/stable-diffusion-public-release
percy_liang,Percy Liang,"Associate Professor, Stanford; Director, Center for Research on Foundation Models",Academic,Seminal Thinker,Stanford University; Stanford HAI,CRFM report 'On the Opportunities and Risks of Foundation Models' (2021); work on holistic evaluation of language models; Stanford HAI annual reports,"Leads systematic study of foundation models, emphasising rigorous evaluation across capabilities, harms, and societal impact; bridges technical research and policy.","'Foundation models are not just another tool, they are a new abstraction in computing.'; 'We need holistic evaluation that covers not only what models can do, but what they should not do.'","Argues for open benchmarks, transparent documentation, and interdisciplinary research on foundation models; sees evaluation infrastructure as public goods; warns about entrenching harms and power imbalances.",Foundation model cartographer; cautious but optimistic under the right guardrails.,Moved from core NLP and robustness work into leading interdisciplinary efforts on foundation model analysis and governance.,Foundation Model Cartographer,Primary,Societal Impact Analyst,Primary,Platform-Level Enterprise Advisor,Secondary,Standards and Evaluation Governance,Primary,Task and Skill Shift Analyst,Secondary,https://crfm.stanford.edu/report.html;https://hai.stanford.edu,On the Opportunities and Risks of Foundation Models,Center for Research on Foundation Models (CRFM),2021-08-01,https://crfm.stanford.edu/report.html